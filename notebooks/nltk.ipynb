{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_map(pos):\n",
    "    _pos = pos[0]\n",
    "    if (_pos == \"J\"): return wn.ADJ\n",
    "    elif (_pos == \"V\"): return wn.VERB\n",
    "    elif (_pos == \"R\"): return wn.ADV\n",
    "    else: return wn.NOUN\n",
    "    \n",
    "# input word_tokenize of sentence\n",
    "def preprocessing(tokens, debug = False):\n",
    "    tokenList = []\n",
    "    tagList = dict()\n",
    "    lemmaList = dict()\n",
    "    \n",
    "    # lemmatize\n",
    "    _lemma = WordNetLemmatizer()\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if (debug):\n",
    "            print(\"token: {}, tag: {}\".format(token, tag))\n",
    "        tokenList.append(token)\n",
    "        tagList[token] = tag\n",
    "\n",
    "        lemma = _lemma.lemmatize(token, tag_map(tag))\n",
    "        lemmaList[token] = lemma\n",
    "        if (debug):\n",
    "            print(token, \"=>\", lemma)\n",
    "    return tokenList, tagList, lemmaList\n",
    "\n",
    "def get_name_pos_from_syn(syn):\n",
    "    name = syn.name().split('.')[0]\n",
    "    pos = syn.name().split('.')[1]\n",
    "    return name, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synDict(tokenList, tagList, lemmaList, debug = False, thres = -1):\n",
    "    synDict = dict()\n",
    "    \n",
    "    # make synset using lemma\n",
    "    for token in tokenList:\n",
    "        lemma = lemmaList[token]\n",
    "        synDict[lemma] = wn.synsets(str(lemma))\n",
    "\n",
    "    # get synset with only same postag\n",
    "    for token in tokenList:\n",
    "        lemma = lemmaList[token]\n",
    "        for val in synDict[lemma]:\n",
    "            name = val.name().split('.')[0]\n",
    "            pos = val.name().split('.')[1]\n",
    "            if (tag_map(tagList[token]) != pos):\n",
    "                synDict[lemma].remove(val)\n",
    "                if (debug):\n",
    "                    print(\"[REMOVE] pos: {}, tokenpos: {} / token: {}, synName: {}\".format(pos, tagList[token], lemma, name))\n",
    "            if (tag_map(tagList[token]) == pos and debug):\n",
    "                print(\"[NOT REMOVE] pos: {}, tokenpos: {} / token: {}, synName: {}\".format(pos, tagList[token], lemma, name))\n",
    "            if (len(synDict[lemma]) == 0):\n",
    "                del(synDict[lemma])\n",
    "                \n",
    "    if (thres > 0):   \n",
    "        print(\"Apply threshold\")\n",
    "        for token in tokenList:\n",
    "            lemma = lemmaList[token]\n",
    "            if (len(synDict[lemma]) >= thres):\n",
    "                del(synDict[lemma])\n",
    "        if (debug): \n",
    "            print(synDict)\n",
    "            \n",
    "    return synDict\n",
    "\n",
    "def get_tense(syn, original_pos):\n",
    "    \n",
    "    # VB: Verb, base form \n",
    "    # VBD: Verb, past tense\n",
    "    # VBG: Verb, gerund or present participle\n",
    "    # VBN: Verb, past participle\n",
    "    # VBP: Verb, non-3rd person singular present\n",
    "    # VBZ: Verb, 3rd person singular present\n",
    "    \n",
    "    name, pos = get_name_pos_from_syn(syn)\n",
    "    _tense = \"present\"            # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "    _person = 1                # 1, 2, 3, or None\n",
    "    _number = \"singular\"       # SG, PL\n",
    "    _mood = \"indicative\"       # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "    _aspect = \"imperfective\"   # IMPERFECTIVE, PERFECTIVE, PROGRESSIVE\n",
    "    \n",
    "    if (original_pos == \"VBD\"):\n",
    "        _tense = \"past\"\n",
    "    elif (original_pos == \"VBG\"):\n",
    "        _aspect = \"progressive\"\n",
    "    elif (original_pos == \"VBN\"):\n",
    "        _tense = \"past\"\n",
    "        _aspect = \"progressive\"\n",
    "    elif (original_pos == \"VBZ\"):\n",
    "        _person = 3\n",
    "    \n",
    "    return conjugate(name,\n",
    "             tense = _tense,\n",
    "             person = _person,\n",
    "             number = _number,\n",
    "             mood = _mood,\n",
    "             aspect = _aspect, \n",
    "             negated = False)\n",
    "    \n",
    "    \n",
    "def make_hypernymDict(synDict, tokenList, tagList, lemmaList, debug=False, thres = -1):\n",
    "    for token in tokenList:\n",
    "        lemma = lemmaList[token]\n",
    "        for syn in synDict[lemma]:\n",
    "            hyper = syn.hyponyms()\n",
    "            if ((thres > 0) and (len(hyper) > thres)):\n",
    "                synDict[lemma].remove(syn)\n",
    "            elif (len(hyper) == 0):\n",
    "                synDict[lemma].remove(syn)\n",
    "            else:\n",
    "                #print(\"lemma {} - hyper of {} is {}\".format(lemma, syn, hyper))\n",
    "                continue\n",
    "                \n",
    "    _key = list(synDict.keys())\n",
    "    if (len(_key) > 1):\n",
    "        rand1 = random.randint(1, len(_key))\n",
    "    else:\n",
    "        rand1 = 1\n",
    "    # print(\"rand1: \", rand1)\n",
    "    keyElem = _key[rand1-1]\n",
    "    randList = synDict[keyElem]\n",
    "    if (len(randList) > 1):\n",
    "        rand2 = random.randint(1, len(randList))\n",
    "    else:\n",
    "        rand2 = 1\n",
    "    # print(\"rand2: \", rand2)\n",
    "    # print(\"randList: \", randList)\n",
    "    randElem = randList[rand2-1]\n",
    "    \n",
    "    # get original word [_token] and original tag [original_tag]\n",
    "    _index = (list(lemmaList.values())).index(keyElem)\n",
    "    _token = tokenList[_index]\n",
    "    original_tag = tagList[_token]\n",
    "    \n",
    "    name, pos = get_name_pos_from_syn(randElem)\n",
    "    renamed = \"\"\n",
    "    if (pos == \"v\"):\n",
    "        # need to take care of tense\n",
    "        _renamed = get_tense(randElem, original_tag)\n",
    "    else:\n",
    "        _renamed = name\n",
    "        \n",
    "    print(\"change {} to {}\".format(_token, _renamed))\n",
    "    \n",
    "    return (_token, _renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def main(sen):\n",
    "    _debug = False\n",
    "    tokens = word_tokenize(sen)\n",
    "    tokenList, tagList, lemmaList = preprocessing(tokens, debug=_debug)\n",
    "    synDict = make_synDict(tokenList, tagList, lemmaList, debug=False)\n",
    "    hypernymDict = make_hypernymDict(synDict, tokenList, tagList, lemmaList, debug=False, thres = 10)\n",
    "main(\"I am planning to have a trip to LA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pattern/text/__init__.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(path, encoding, comment)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e9ea0e7ecf31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I am planning to have a trip to LA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e9ea0e7ecf31>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(sen)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmaList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msynDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_synDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmaList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mori_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnxt_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_hypernymDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmaList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mretVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnxt_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mretVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c135f04976c5>\u001b[0m in \u001b[0;36mmake_hypernymDict\u001b[0;34m(synDict, tokenList, tagList, lemmaList, debug, thres)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# need to take care of tense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0m_renamed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandElem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0m_renamed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c135f04976c5>\u001b[0m in \u001b[0;36mget_tense\u001b[0;34m(syn, original_pos)\u001b[0m\n\u001b[1;32m     65\u001b[0m              \u001b[0mmood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mood\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m              \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_aspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m              negated = False)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pattern/text/__init__.py\u001b[0m in \u001b[0;36mconjugate\u001b[0;34m(self, verb, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2206\u001b[0m         \u001b[0mi2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m         \u001b[0mi3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2208\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2209\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m         \u001b[0;31m# Get the verb lexeme and return the requested index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pattern/text/__init__.py\u001b[0m in \u001b[0;36mlemma\u001b[0;34m(self, verb, parse)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \"\"\"\n\u001b[1;32m   2171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pattern/text/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;31m# have,,,has,,having,,,,,had,had,haven't,,,hasn't,,,,,,,hadn't,hadn't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTENSES_ID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINFINITIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "def main(sen): \n",
    "    debug = False \n",
    "    tokens = word_tokenize(sen)\n",
    "    tokenList, tagList, lemmaList = preprocessing(tokens, debug=debug)\n",
    "    synDict = make_synDict(tokenList, tagList, lemmaList, debug=False) \n",
    "    (ori_word, nxt_word) = make_hypernymDict(synDict, tokenList, tagList, lemmaList, debug=False, thres = 10) \n",
    "    retVal = sen.replace(ori_word, nxt_word)\n",
    "    return retVal\n",
    "    \n",
    "for i in range(10):\n",
    "    print(main(\"I am planning to have a trip to LA\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
