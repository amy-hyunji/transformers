nsml run -d cache_bert_txt -c 2 -g 3 --memory '100G' -m "transformer original bert dataset" -e run_language_modeling.py -a "--tokenizer_name=bert-base-uncased --fp16 --output_dir=output --do_train --train_data_file=/data/cache_bert_txt/train/cached_lm_BertTokenizerFast_126_bert_train.txt --model_type=electra --per_device_train_batch_size=64 --num_train_epoch=40 --save_steps=10000 --max_steps=1000000 --block_size=128" 
